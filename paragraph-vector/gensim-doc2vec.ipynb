{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gensim Doc2Vec\n",
    "\n",
    "모든 문장의 vector를 구한 다음 가장 비슷한 vector를 가진 문장을 찾는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pickle\n",
    "import os\n",
    "from nltk import word_tokenize\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from gensim.test.utils import get_tmpfile\n",
    "\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pickle 파일이 존재하지 않으면 S3에서 문장 데이터를 다운받아 pickle 파일로 저장한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5432e8a79ca64b3e9616a94e01e78ab1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=12876), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11849 files downloaded. 6903759 sentences saved.\n"
     ]
    }
   ],
   "source": [
    "bucket = boto3.resource('s3').Bucket('learningdatajchswm9')\n",
    "\n",
    "# Set up a corpus\n",
    "common_texts = []\n",
    "pickle_file = 'corpus.pickle'\n",
    "# pickle 파일이 이미 존재하면\n",
    "if not os.path.exists(pickle_file):\n",
    "    download_cnt = 0\n",
    "    # temp_file = 's3_download_temp.txt'\n",
    "    local_file = 'sentences/HOO' + str(i) + 'content.txt'\n",
    "\n",
    "    total = tqdm_notebook(range(12876))\n",
    "    for i in total:\n",
    "    #for i in range(12876):\n",
    "        try:\n",
    "            if not os.path.exists(local_file):\n",
    "                bucket.download_file(local_file, local_file)\n",
    "            with open(local_file, 'r', encoding='UTF-8') as f:\n",
    "                sentences = f.read().splitlines()\n",
    "                common_texts += [word_tokenize(sent) for sent in sentences]\n",
    "            download_cnt += 1\n",
    "            # print(i, 'downloaded.')\n",
    "\n",
    "        except Exception as e:\n",
    "            pass\n",
    "            # print(i, e)\n",
    "\n",
    "    os.remove(temp_file)\n",
    "    print('%d files downloaded. %d sentences saved.' % (download_cnt, len(common_texts)))\n",
    "\n",
    "    with open(pickle_file, 'wb') as f:\n",
    "        pickle.dump(common_texts, f, pickle.HIGHEST_PROTOCOL)\n",
    "# pickle 파일이 존재하지 않으면\n",
    "else:\n",
    "    with open(pickle_file, 'rb') as f:\n",
    "        common_texts = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6903759"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(common_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델을 학습시키거나 학습된 모델을 가져온다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning on? (y/n)...y\n"
     ]
    }
   ],
   "source": [
    "model_file = get_tmpfile(\"my_doc2vec_model\")\n",
    "learning_on = input('Learning on? (y/n)...').lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if learning_on == 'y':\n",
    "    # Initialize & train a model\n",
    "    documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(common_texts)]\n",
    "    model = Doc2Vec(documents, vector_size=300, window=4, min_count=3, workers=4, epochs=15, dbow_words=1)\n",
    "    # print('documents =', documents)\n",
    "\n",
    "    # Persist a model to disk\n",
    "    model.save(model_file)\n",
    "else:\n",
    "    model = Doc2Vec.load(model_file)\n",
    "# Delete temporary training data\n",
    "model.delete_temporary_training_data()\n",
    "model_parameters = 'vector_size=%d,window=%d,min_count=%d,epochs=%d,dbow_words=1' % (model.vector_size, model.window, model.vocabulary.min_count, model.epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "사용자 입력을 받아 입력 문장과 가장 유사한 문장을 보여준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_sent = word_tokenize(input('User typing : '))\n",
    "# input_sent = word_tokenize('Software is becoming an increasingly large part in the automotive industry')\n",
    "# print(input_sent)\n",
    "\n",
    "# input_sent를 여러개 만들자\n",
    "input_sents = ['Software is becoming an increasingly large part in the automotive industry',\n",
    "               'We present a novel approach to modeling stories using recurrent neural networks',\n",
    "              'Software for the module is a part of operating system']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "same_cnt = 0\n",
    "\n",
    "for input_sent in input_sents:\n",
    "    input_sent = word_tokenize(input_sent)\n",
    "    # Infer a vector for a new document\n",
    "    # vector = model.infer_vector(input_sent)\n",
    "    # infer vector 평균해볼까\n",
    "    vector = 0\n",
    "    vector_average_n = 100\n",
    "    for i in range(vector_average_n):\n",
    "        vector += model.infer_vector(input_sent)\n",
    "    vector = vector / vector_average_n\n",
    "    # print(vector)\n",
    "\n",
    "    # Search for the most similar sentences\n",
    "    similar_sentences = model.docvecs.most_similar(positive=[vector], topn=5)\n",
    "    #print('Top 5 most similar sentences are:')\n",
    "    for pair in similar_sentences:\n",
    "        sent_num = pair[0]\n",
    "        similarity = pair[1]\n",
    "        # print(common_texts[sent_num], similarity)\n",
    "\n",
    "    # Log\n",
    "    log_path = 'logs/' + model_parameters + '.txt'\n",
    "    with open(log_path, 'a+') as f:\n",
    "        f.write('input_sent:' + str(input_sent) + '\\n')\n",
    "        for pair in similar_sentences:\n",
    "            sent_num = pair[0]\n",
    "            similarity = pair[1]\n",
    "            f.write(str(common_texts[sent_num]) + str(similarity) + '\\n')\n",
    "        f.write('\\n')\n",
    "\n",
    "    # top 5에 같은 문장이 몇 개 나왔는지 센다.\n",
    "    for pair in similar_sentences:\n",
    "            sent_num = pair[0]\n",
    "            similarity = pair[1]\n",
    "            if common_texts[sent_num] == input_sent:\n",
    "                same_cnt += 1\n",
    "print(same_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
