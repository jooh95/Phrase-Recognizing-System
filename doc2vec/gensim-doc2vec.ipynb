{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gensim Doc2Vec\n",
    "\n",
    "모든 문장의 vector를 구한 다음 가장 비슷한 vector를 가진 문장을 찾는다.\n",
    "\n",
    "1. 전체 문장을 소문자화 한다.\n",
    "2. 알파벳으로만 된 단어 개수가 5개 이상, 단어 개수가 15개 이하인 문장만 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning on? (y/n)...n\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import pickle\n",
    "import os\n",
    "import re\n",
    "from nltk import word_tokenize\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from gensim.test.utils import get_tmpfile\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "model_file = 'doc2vec.model'\n",
    "pickle_file = 'corpus.pickle'\n",
    "learning_on = input('Learning on? (y/n)...').lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pickle 파일이 존재하지 않으면 S3에서 문장 데이터를 다운받아 pickle 파일로 저장한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwrite? (y/n)...n\n"
     ]
    }
   ],
   "source": [
    "p = re.compile('[a-z]+')  # 알파벳으로만 된 단어 패턴\n",
    "common_texts = []\n",
    "\n",
    "overwrite = 'n'\n",
    "if os.path.exists(pickle_file):  # pickle 파일이 존재하면\n",
    "    overwrite = input('Overwrite? (y/n)...').lower()\n",
    "\n",
    "if overwrite == 'y' or not os.path.exists(pickle_file):\n",
    "    bucket = boto3.resource('s3').Bucket('learningdatajchswm9')\n",
    "    download_cnt = 0\n",
    "\n",
    "    for i in tqdm_notebook(range(12876)):\n",
    "        # local_file = 'sents/HOO' + str(i) + 'abstract.txt'\n",
    "        local_file = 'sents/HOO' + str(i) + 'content.txt'\n",
    "        \n",
    "        try:\n",
    "            if not os.path.exists(local_file):\n",
    "                bucket.download_file(local_file, local_file)\n",
    "                \n",
    "            with open(local_file, 'r', encoding='UTF-8') as f:\n",
    "                sentences = f.read().splitlines()\n",
    "                for sent in sentences:\n",
    "                    tokenized = word_tokenize(sent.lower())\n",
    "                    common_texts.append(tokenized)\n",
    "                        \n",
    "            download_cnt += 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(i, e)\n",
    "\n",
    "    with open(pickle_file, 'wb') as f:\n",
    "        pickle.dump(common_texts, f, pickle.HIGHEST_PROTOCOL)\n",
    "    print('%d files downloaded. %d sentences saved.' % (download_cnt, len(common_texts)))\n",
    "else:\n",
    "    with open(pickle_file, 'rb') as f:\n",
    "        common_texts = pickle.load(f)\n",
    "        print(len(common_texts), 'sentences loaded.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델을 학습시키거나 학습된 모델을 가져온다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "if learning_on == 'y':\n",
    "    # Initialize & train a model\n",
    "    documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(common_texts)]\n",
    "    model = Doc2Vec(documents, vector_size=300, window=7, min_count=20, workers=8, epochs=25, dm=1, dm_mean=1, dbow_words=1)\n",
    "\n",
    "    model.save(model_file)\n",
    "    \n",
    "else:\n",
    "    model = Doc2Vec.load(model_file)\n",
    "    \n",
    "model_parameters = 'vector_size=%d,window=%d,min_count=%d,epochs=%d' % (model.vector_size, model.window, model.vocabulary.min_count, model.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.delete_temporary_training_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "input_sents = ['Software is becoming an increasingly large part in the automotive industry.',\n",
    "               'Experimental results on real and synthetic datasets demonstrate the effectiveness of our model.',\n",
    "              'This thesis introduces dynamic software updating.',\n",
    "              'We also describe a variant that scales to high-dimensional domains.',\n",
    "              'Cloud computing faces many problems.',\n",
    "              'update',\n",
    "              'We consider that we can face problems',\n",
    "              'We faces many problems',\n",
    "              'We are developing something good.']\n",
    "\n",
    "for input_sent in input_sents:\n",
    "    input_sent = word_tokenize(input_sent.lower())\n",
    "    \n",
    "    average_n = 10\n",
    "    vector = 0\n",
    "    for k in range(average_n):\n",
    "        vector += model.infer_vector(input_sent)\n",
    "    vector /= average_n\n",
    "\n",
    "    # Search for the most similar sentences\n",
    "    similar_sentences = model.docvecs.most_similar(positive=[vector], topn=10)\n",
    "\n",
    "    # topn개의 문장 중에서 이상한 문장 지우기\n",
    "    good_sents = []\n",
    "    for sent_index in range(len(similar_sentences)):\n",
    "        sent = similar_sentences[sent_index]\n",
    "        is_good_sent = True\n",
    "        # 문장의 단어 수가 5보다 작고\n",
    "        if len(sent) < 5:\n",
    "            is_good_sent = False\n",
    "        # 한 단어의 길이가 20을 넘지 않고, 영어가 아닌 문장이 너무 많이 들어가 있지 않아야\n",
    "        good_word_cnt = 0\n",
    "        for word in sent:\n",
    "            if len(word) >= 20:\n",
    "                is_good_sent = False\n",
    "                break\n",
    "            if p.match(word):\n",
    "                good_word_cnt += 1\n",
    "        if good_word_cnt < 0.5 * len(sent):\n",
    "            is_good_sent = False\n",
    "            \n",
    "        if is_good_sent:\n",
    "            good_sents.append(sent)\n",
    "    \n",
    "    # Log\n",
    "    log_path = 'logs/' + model_parameters + '.txt'\n",
    "    with open(log_path, 'a+', encoding='UTF-8') as f:\n",
    "        for word in input_sent:\n",
    "            f.write(word + ' ')\n",
    "        f.write('<- input_sent\\n')\n",
    "        for sent_num, similarity in similar_sentences:\n",
    "            for word in common_texts[sent_num]:\n",
    "                f.write(word + ' ')\n",
    "            f.write(str(similarity) + '\\n')\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bangjungyip\\AppData\\Local\\conda\\conda\\envs\\gensim\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('explore', 0.7097293734550476),\n",
       " ('outline', 0.69687819480896),\n",
       " ('discuss', 0.6799103617668152),\n",
       " ('introduce', 0.6348254680633545),\n",
       " ('investigate', 0.6238253116607666),\n",
       " ('illustrate', 0.5804440975189209),\n",
       " ('implement', 0.5782642364501953),\n",
       " ('analyze', 0.5741323232650757),\n",
       " ('employ', 0.572858452796936),\n",
       " ('deﬁne', 0.5725528001785278)]"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(positive=['describe'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "문장을 전체 돌면서 유사도가 가장 높은 문장을 뽑아서 보여주기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e0619b2e75241ecb41489e85f9e40fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=31586), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "similarity = [0.0] * len(common_texts)\n",
    "\n",
    "for i in tqdm_notebook(range(len(common_texts))):\n",
    "    # 해당 문장의 vector 값 추론\n",
    "    input_sent = common_texts[i]\n",
    "    vector = 0\n",
    "    vector_average_n = 5\n",
    "    for j in range(vector_average_n):\n",
    "        vector += model.infer_vector(input_sent)\n",
    "    vector = vector / vector_average_n\n",
    "    \n",
    "    # Search for the most similar sentences\n",
    "    similar_sentences = model.docvecs.most_similar(positive=[vector], topn=5)\n",
    "    average = 0.0\n",
    "    cnt = 0\n",
    "    for j, simil_point in similar_sentences:\n",
    "        if common_texts[j] != input_sent:\n",
    "            average += simil_point\n",
    "    if cnt:\n",
    "        average = average / cnt\n",
    "    \n",
    "    similarity[i] = average\n",
    "    \n",
    "sorted_by_similarity = sorted(enumerate(similarity), key=lambda tup: tup[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "for tup in sorted_by_similarity[1000:1100]:\n",
    "    input_sent = common_texts[tup[0]]\n",
    "    vector = 0\n",
    "    vector_average_n = 5\n",
    "    for i in range(vector_average_n):\n",
    "        vector += model.infer_vector(input_sent)\n",
    "    vector = vector / vector_average_n\n",
    "    \n",
    "    # Search for the most similar sentences\n",
    "    similar_sentences = model.docvecs.most_similar(positive=[vector], topn=5)\n",
    "    \n",
    "    # Log\n",
    "    log_path = 'logs/' + model_parameters + '.txt'\n",
    "    with open(log_path, 'a+', encoding='UTF-8') as f:\n",
    "        for word in input_sent:\n",
    "            f.write(word + ' ')\n",
    "        f.write('<- input_sent\\n')\n",
    "        for pair in similar_sentences:\n",
    "            sent_num = pair[0]\n",
    "            similarity = pair[1]\n",
    "            for word in common_texts[sent_num]:\n",
    "                f.write(word + ' ')\n",
    "            f.write('\\n')\n",
    "        f.write('\\n')\n",
    "        \n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "랜덤 문장을 선택해 그것과 가장 비슷한 문장을 뽑아주는 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "num_sents = len(common_texts)  # 문장의 총 개수\n",
    "\n",
    "for i in range(5):\n",
    "    randn = random.randrange(num_sents)\n",
    "    input_sent = common_texts[randn]\n",
    "    \n",
    "    vector = 0\n",
    "    vector_average_n = 100\n",
    "    for i in range(vector_average_n):\n",
    "        vector += model.infer_vector(input_sent)\n",
    "    vector = vector / vector_average_n\n",
    "        \n",
    "    # Search for the most similar sentences\n",
    "    similar_sentences = model.docvecs.most_similar(positive=[vector], topn=5)\n",
    "\n",
    "    # Log\n",
    "    log_path = 'logs/' + model_parameters + '.txt'\n",
    "    with open(log_path, 'a+', encoding='UTF-8') as f:\n",
    "        for word in input_sent:\n",
    "            f.write(word + ' ')\n",
    "        f.write('<- input_sent\\n')\n",
    "        for sent_num, similarity in similar_sentences:\n",
    "            for word in common_texts[sent_num]:\n",
    "                f.write(word + ' ')\n",
    "            f.write('\\n')\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
