{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gensim Doc2Vec\n",
    "\n",
    "모든 문장의 vector를 구한 다음 가장 비슷한 vector를 가진 문장을 찾는다.\n",
    "\n",
    "1. 전체 문장을 소문자화 한다.\n",
    "2. 알파벳으로만 된 단어 개수가 5개 이상, 단어 개수가 15개 이하인 문장만 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bangjungyip\\AppData\\Local\\conda\\conda\\envs\\gensim\\lib\\site-packages\\gensim\\utils.py:1209: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning on? (y/n)...y\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import pickle\n",
    "import os\n",
    "import re\n",
    "from nltk import word_tokenize\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from gensim.test.utils import get_tmpfile\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "model_file = 'doc2vec.model'\n",
    "pickle_file = 'corpus.pickle'\n",
    "learning_on = input('Learning on? (y/n)...').lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('txtData/'):\n",
    "    os.mkdir('txtData')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pickle 파일이 존재하지 않으면 S3에서 문장 데이터를 다운받아 pickle 파일로 저장한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwrite? (y/n)...y\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d50770aa9a147eb90bea9e9ea8adbf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=12876), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "186 files downloaded. 179767 sentences saved.\n"
     ]
    }
   ],
   "source": [
    "p = re.compile('[a-zA-Z]+')  # 알파벳으로만 된 단어 패턴\n",
    "\n",
    "original_sents = []\n",
    "common_texts = []\n",
    "\n",
    "overwrite = 'n'\n",
    "if os.path.exists(pickle_file):  # pickle 파일이 존재하면\n",
    "    overwrite = input('Overwrite? (y/n)...').lower()\n",
    "\n",
    "if overwrite == 'y' or not os.path.exists(pickle_file):\n",
    "    bucket = boto3.resource('s3').Bucket('learningdatajchswm9')\n",
    "    download_cnt = 0\n",
    "\n",
    "    for i in tqdm_notebook(range(12876)):\n",
    "        # local_file = 'sents/HOO' + str(i) + 'abstract.txt'\n",
    "        # local_file = 'sents/HOO' + str(i) + 'content.txt'\n",
    "        local_file = 'txtData/HOO' + str(i) + '.txt'\n",
    "        \n",
    "        try:\n",
    "            if not os.path.exists(local_file):\n",
    "                bucket.download_file(local_file, local_file)\n",
    "                \n",
    "            with open(local_file, 'r', encoding='UTF-8') as f:\n",
    "                sentences = f.read().splitlines()\n",
    "                original_sents += sentences\n",
    "                for sent in sentences:\n",
    "                    tokenized = word_tokenize(sent.lower())\n",
    "                    common_texts.append(tokenized)\n",
    "                        \n",
    "            download_cnt += 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            # print(i, e)\n",
    "            pass\n",
    "\n",
    "    with open(pickle_file, 'wb') as f:\n",
    "        pickle.dump(common_texts, f, pickle.HIGHEST_PROTOCOL)\n",
    "    print('%d files downloaded. %d sentences saved.' % (download_cnt, len(common_texts)))\n",
    "else:\n",
    "    with open(pickle_file, 'rb') as f:\n",
    "        common_texts = pickle.load(f)\n",
    "        print(len(common_texts), 'sentences loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identifying Student Difficulties with Basic Data Structures Daniel Zingaro University of Toronto Mississauga daniel.zingaro@utoronto.ca Michael Clancy University of California, Berkeley Cynthia Taylor Oberlin College ctaylor@oberlin.edu Cynthia Lee Stanford University Kevin C. Webb Swarthmore College Leo Porter University of California, San Diego leporter@eng.ucsd.edu Soohyun Nam Liao University of California, San Diego ABSTRACT To be effective instructors and CS education researchers, we must identify and understand student difficulties surrounding core computing topics.\n",
      "This study examines student difficulties with the basic data structures commonly found in CS2 courses.\n",
      "Initial exploration of student thinking began with think-aloud interviews with students.\n",
      "These interviews centered on open-ended questions that were iteratively improved upon based on analysis of interview transcripts.\n",
      "The revised open-ended questions were then posed to 249 students during an end-of-term final exam study session.\n",
      "Using the explanations and justifications included by students, responses to the questions were coded and summarized.\n",
      "This work characterizes the difficulties revealed by student responses, and provides details of their prevalence among the examined student population.\n",
      "CCS CONCEPTS • Social and professional topics → Computing Education; KEYWORDS CS2, data structures, difficulties ACM Reference Format: Daniel Zingaro, Cynthia Taylor, Leo Porter, Michael Clancy, Cynthia Lee, Soohyun Nam Liao, and Kevin C. Webb.\n",
      "2018.\n",
      "Identifying Student Difficulties with Basic Data Structures.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "179767"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for sent in original_sents[:10]:\n",
    "    print(sent)\n",
    "\n",
    "len(original_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델을 학습시키거나 학습된 모델을 가져온다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "if learning_on == 'y':\n",
    "    # Initialize & train a model\n",
    "    documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(common_texts)]\n",
    "    model = Doc2Vec(documents, vector_size=300, window=7, min_count=10, workers=8, epochs=25, dm=1, dm_mean=1, dbow_words=1)\n",
    "\n",
    "    model.save(model_file)\n",
    "    \n",
    "else:\n",
    "    model = Doc2Vec.load(model_file)\n",
    "    \n",
    "model_parameters = 'vector_size=%d,window=%d,min_count=%d,epochs=%d' % (model.vector_size, model.window, model.vocabulary.min_count, model.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.delete_temporary_training_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 285 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "input_sents = ['Software is becoming an increasingly large part in the automotive industry.',\n",
    "               'Experimental results on real and synthetic datasets demonstrate the effectiveness of our model.',\n",
    "              'This thesis introduces dynamic software updating.',\n",
    "              'We also describe a variant that scales to high-dimensional domains.',\n",
    "              'Cloud computing faces many problems.',\n",
    "              'update',\n",
    "              'We consider that we can face problems',\n",
    "              'We faces many problems',\n",
    "              'We are developing something good.']\n",
    "\n",
    "for input_sent in input_sents:\n",
    "    input_token = word_tokenize(input_sent.lower())\n",
    "    \n",
    "    average_n = 10\n",
    "    vector = 0\n",
    "    for k in range(average_n):\n",
    "        vector += model.infer_vector(input_token)\n",
    "    vector /= average_n\n",
    "\n",
    "    # Search for the most similar sentences\n",
    "    similar_sentences = model.docvecs.most_similar(positive=[vector], topn=10)\n",
    "    \n",
    "    # topn개의 문장 중에서 이상한 문장 지우기\n",
    "    good_sents = []\n",
    "    for sent_index in range(len(similar_sentences)):\n",
    "        sent = original_sents[similar_sentences[sent_index][0]]\n",
    "        is_good_sent = True\n",
    "        # 문장의 단어 수가 5보다 작고\n",
    "        if len(sent) < 5:\n",
    "            is_good_sent = False\n",
    "        # 한 단어의 길이가 20을 넘지 않고, 영어가 아닌 문장이 너무 많이 들어가 있지 않아야\n",
    "        good_word_cnt = 0\n",
    "        for word in sent:\n",
    "            if len(word) >= 20:\n",
    "                is_good_sent = False\n",
    "                break\n",
    "            if p.match(word):\n",
    "                good_word_cnt += 1\n",
    "        if good_word_cnt < 0.5 * len(sent):\n",
    "            is_good_sent = False\n",
    "            \n",
    "        if is_good_sent:\n",
    "            good_sents.append(similar_sentences[sent_index])\n",
    "\n",
    "    show_cnt = 0  # 3개 문장만 보여줄거다.\n",
    "    # Log\n",
    "    log_path = 'logs/' + model_parameters + '.txt'\n",
    "    with open(log_path, 'a+', encoding='UTF-8') as f:\n",
    "        f.write(str(input_sent) + ' <- input_sent\\n')\n",
    "        for sent_num, similarity in good_sents:\n",
    "            if similarity < 0.5:\n",
    "                break\n",
    "            f.write(str(original_sents[sent_num]) + ' ' + str(similarity) + '\\n')\n",
    "            show_cnt += 1\n",
    "            if show_cnt >= 3:  # 3개 문장만 보여줄거다.\n",
    "                break\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bangjungyip\\AppData\\Local\\conda\\conda\\envs\\gensim\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('define', 0.6457910537719727),\n",
       " ('explain', 0.6221498847007751),\n",
       " ('explore', 0.5916944742202759),\n",
       " ('discuss', 0.591374933719635),\n",
       " ('examine', 0.5906602740287781),\n",
       " ('deﬁne', 0.5837196111679077),\n",
       " ('investigate', 0.5834907293319702),\n",
       " ('illustrate', 0.5696419477462769),\n",
       " ('outline', 0.5407627820968628),\n",
       " ('identify', 0.5361859202384949)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(positive=['describe'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "문장을 전체 돌면서 유사도가 가장 높은 문장을 뽑아서 보여주기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity = [0.0] * len(common_texts)\n",
    "\n",
    "for i in tqdm_notebook(range(len(common_texts))):\n",
    "    # 해당 문장의 vector 값 추론\n",
    "    input_sent = common_texts[i]\n",
    "    vector = 0\n",
    "    vector_average_n = 5\n",
    "    for j in range(vector_average_n):\n",
    "        vector += model.infer_vector(input_sent)\n",
    "    vector = vector / vector_average_n\n",
    "    \n",
    "    # Search for the most similar sentences\n",
    "    similar_sentences = model.docvecs.most_similar(positive=[vector], topn=5)\n",
    "    average = 0.0\n",
    "    cnt = 0\n",
    "    for j, simil_point in similar_sentences:\n",
    "        if common_texts[j] != input_sent:\n",
    "            average += simil_point\n",
    "    if cnt:\n",
    "        average = average / cnt\n",
    "    \n",
    "    similarity[i] = average\n",
    "    \n",
    "sorted_by_similarity = sorted(enumerate(similarity), key=lambda tup: tup[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tup in sorted_by_similarity[1000:1100]:\n",
    "    input_sent = common_texts[tup[0]]\n",
    "    vector = 0\n",
    "    vector_average_n = 5\n",
    "    for i in range(vector_average_n):\n",
    "        vector += model.infer_vector(input_sent)\n",
    "    vector = vector / vector_average_n\n",
    "    \n",
    "    # Search for the most similar sentences\n",
    "    similar_sentences = model.docvecs.most_similar(positive=[vector], topn=5)\n",
    "    \n",
    "    # Log\n",
    "    log_path = 'logs/' + model_parameters + '.txt'\n",
    "    with open(log_path, 'a+', encoding='UTF-8') as f:\n",
    "        for word in input_sent:\n",
    "            f.write(word + ' ')\n",
    "        f.write('<- input_sent\\n')\n",
    "        for pair in similar_sentences:\n",
    "            sent_num = pair[0]\n",
    "            similarity = pair[1]\n",
    "            for word in common_texts[sent_num]:\n",
    "                f.write(word + ' ')\n",
    "            f.write('\\n')\n",
    "        f.write('\\n')\n",
    "        \n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "랜덤 문장을 선택해 그것과 가장 비슷한 문장을 뽑아주는 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "num_sents = len(common_texts)  # 문장의 총 개수\n",
    "\n",
    "for i in range(5):\n",
    "    randn = random.randrange(num_sents)\n",
    "    input_sent = common_texts[randn]\n",
    "    \n",
    "    vector = 0\n",
    "    vector_average_n = 100\n",
    "    for i in range(vector_average_n):\n",
    "        vector += model.infer_vector(input_sent)\n",
    "    vector = vector / vector_average_n\n",
    "        \n",
    "    # Search for the most similar sentences\n",
    "    similar_sentences = model.docvecs.most_similar(positive=[vector], topn=5)\n",
    "\n",
    "    # Log\n",
    "    log_path = 'logs/' + model_parameters + '.txt'\n",
    "    with open(log_path, 'a+', encoding='UTF-8') as f:\n",
    "        for word in input_sent:\n",
    "            f.write(word + ' ')\n",
    "        f.write('<- input_sent\\n')\n",
    "        for sent_num, similarity in similar_sentences:\n",
    "            for word in common_texts[sent_num]:\n",
    "                f.write(word + ' ')\n",
    "            f.write('\\n')\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
