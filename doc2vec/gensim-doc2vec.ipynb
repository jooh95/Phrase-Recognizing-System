{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gensim Doc2Vec\n",
    "\n",
    "모든 문장의 vector를 구한 다음 가장 비슷한 vector를 가진 문장을 찾는다.\n",
    "\n",
    "1. 전체 문장을 소문자화 한다.\n",
    "2. 알파벳으로만 된 단어 개수가 5개 이상, 단어의 글자 수가 20 이하인 문장만 보여준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bangjungyip\\AppData\\Local\\conda\\conda\\envs\\gensim\\lib\\site-packages\\gensim\\utils.py:1209: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import pickle\n",
    "import os\n",
    "import re\n",
    "from nltk import word_tokenize\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from gensim.test.utils import get_tmpfile\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning on? (y/n)...y\n",
      "Overwrite? (y/n)...n\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists('txtData/'):\n",
    "    os.mkdir('txtData')\n",
    "    \n",
    "model_file = 'doc2vec.model'\n",
    "pickle_file = 'corpus.pickle'\n",
    "common_texts_pickle = 'common_texts.pickle'\n",
    "\n",
    "p = re.compile('[a-zA-Z]+')  # 알파벳으로만 된 단어 패턴\n",
    "\n",
    "learning_on = input('Learning on? (y/n)...').lower()\n",
    "\n",
    "overwrite = 'n'\n",
    "if os.path.exists(pickle_file):  # pickle 파일이 존재하면\n",
    "    overwrite = input('Overwrite? (y/n)...').lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pickle 파일이 존재하지 않으면 S3에서 문장 데이터를 다운받아 pickle 파일로 저장한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3722837 sentences loaded.\n"
     ]
    }
   ],
   "source": [
    "original_sents = []\n",
    "common_texts = []\n",
    "\n",
    "if overwrite == 'y' or not os.path.exists(pickle_file):\n",
    "    bucket = boto3.resource('s3').Bucket('learningdatajchswm9')\n",
    "    download_cnt = 0\n",
    "\n",
    "    for i in tqdm_notebook(range(12876)):\n",
    "        local_file = 'txtData/HOO' + str(i) + '.txt'\n",
    "        \n",
    "        try:\n",
    "            if not os.path.exists(local_file):\n",
    "                bucket.download_file(local_file, local_file)\n",
    "                \n",
    "            with open(local_file, 'r', encoding='UTF-8') as f:\n",
    "                for sent in f.read().splitlines():\n",
    "                    sent2 = sent.strip()\n",
    "                    if sent2 != '':\n",
    "                        original_sents.append(sent2)\n",
    "                        common_texts.append(word_tokenize(sent2))\n",
    "                        \n",
    "            download_cnt += 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(i, e)\n",
    "\n",
    "    with open(pickle_file, 'wb') as f:\n",
    "        pickle.dump(original_sents, f, pickle.HIGHEST_PROTOCOL)\n",
    "    with open(common_texts_pickle, 'wb') as f:\n",
    "        pickle.dump(common_texts, f, pickle.HIGHEST_PROTOCOL)\n",
    "    print('%d files downloaded. %d sentences saved.' % (download_cnt, len(original_sents)))\n",
    "else:\n",
    "    with open(pickle_file, 'rb') as f:\n",
    "        original_sents = pickle.load(f)\n",
    "    with open(common_texts_pickle, 'rb') as f:\n",
    "        common_texts = pickle.load(f)\n",
    "    print(len(original_sents), 'sentences loaded.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델을 학습시키거나 학습된 모델을 가져온다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "if learning_on == 'y':\n",
    "    documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(common_texts)]\n",
    "    \n",
    "    # Initialize & train a model\n",
    "    model = Doc2Vec(documents, vector_size=300, window=5, min_count=50, workers=8, dm=1, dm_mean=1, dbow_words=1)\n",
    "    model.train(documents, total_examples=model.corpus_count, epochs=25, start_alpha=0.025, end_alpha=0.001)\n",
    "\n",
    "    model.save(model_file)\n",
    "    \n",
    "else:\n",
    "    model = Doc2Vec.load(model_file)\n",
    "    \n",
    "model_parameters = 'vector_size=%d,window=%d,min_count=%d,epochs=%d' % (model.vector_size, model.window, model.vocabulary.min_count, model.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.delete_temporary_training_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "input_sents = ['Software is becoming an increasingly large part in the automotive industry.',\n",
    "               'Experimental results on real and synthetic datasets demonstrate the effectiveness of our model.',\n",
    "              'This thesis introduces dynamic software updating.',\n",
    "              'We also describe a variant that scales to high-dimensional domains.',\n",
    "              'Cloud computing faces many problems.',\n",
    "              'update',\n",
    "              'We consider that we can face problems',\n",
    "              'We faces many problems',\n",
    "              'We are developing something good.',\n",
    "              'The revised open-ended questions were then posed to 249 students during an end-of-term final exam study session.',\n",
    "              'This paper suggests the use of formal models',\n",
    "              'In this paper, we prove that',\n",
    "              'We showed that it is used to',\n",
    "              'Why do this?',\n",
    "              'In this paper, we focus on the problem of generating adversarial examples for Natural Language Inference (NLI) models in order to gain insights about the inner workings of such systems, and regularising them.']\n",
    "\n",
    "for input_sent in input_sents:\n",
    "    input_token = word_tokenize(input_sent.lower())\n",
    "    \n",
    "    average_n = 10\n",
    "    vector = 0\n",
    "    for k in range(average_n):\n",
    "        vector += model.infer_vector(input_token)\n",
    "    vector /= average_n\n",
    "\n",
    "    # Search for the most similar sentences\n",
    "    similar_sentences = model.docvecs.most_similar(positive=[vector], topn=10)\n",
    "    \n",
    "    # topn개의 문장 중에서 이상한 문장 지우기\n",
    "    good_sents = []\n",
    "    \n",
    "    for sent_index in range(len(similar_sentences)):\n",
    "        sent = original_sents[similar_sentences[sent_index][0]]\n",
    "        \n",
    "        is_good_sent = True\n",
    "        \n",
    "        # 문장의 단어 수가 5보다 작으면 좋은 문장이 아니다.\n",
    "        if len(sent) < 5:\n",
    "            is_good_sent = False\n",
    "            \n",
    "        # 한 단어의 길이가 20을 넘지 않고, 영어가 아닌 문장이 너무 많이 들어가 있지 않아야\n",
    "        good_word_cnt = 0\n",
    "        for word in sent:\n",
    "            if len(word) >= 20:\n",
    "                is_good_sent = False\n",
    "                break\n",
    "            if p.match(word):\n",
    "                good_word_cnt += 1\n",
    "                \n",
    "        # 특수 문자가 너무 많을 경우\n",
    "        if good_word_cnt < 0.67 * len(sent):\n",
    "            is_good_sent = False\n",
    "            \n",
    "        # 이미 있는 문장의 경우\n",
    "        if sent in good_sents:\n",
    "            is_good_sent = False\n",
    "            \n",
    "        if is_good_sent:\n",
    "            good_sents.append(similar_sentences[sent_index])\n",
    "\n",
    "    show_cnt = 0  # 3개 문장만 보여줄거다.\n",
    "    # Log\n",
    "    log_path = 'logs/' + model_parameters + '.txt'\n",
    "    with open(log_path, 'a+', encoding='UTF-8') as f:\n",
    "        f.write(str(input_sent) + ' <- input_sent\\n')\n",
    "        for sent_num, similarity in good_sents:\n",
    "            if similarity < 0.5:\n",
    "                break\n",
    "            f.write(str(original_sents[sent_num]) + ' ' + str(similarity) + '\\n')\n",
    "            show_cnt += 1\n",
    "            if show_cnt >= 3:  # 3개 문장만 보여줄거다.\n",
    "                break\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.most_similar(positive=['describe'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To do\n",
    "## Done\n",
    "- 중복 문장 제거하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
